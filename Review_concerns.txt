Reviewer 1
The topic of the paper is interesting, but the current content but also the presentation haven't reach a stage yet where it can or should be published at a top-tier conference. Simply speaking, the paper feels a bit rushed. More specifically, I have the following comments that I hope will help the authors to improve their work:

C1: I'm not sure about the purpose of the paper. In the authors' own words: "[...] it is to provide a baseline, [...] to provide insights [...]". However, I don't think the authors succeed with the current content. To be a meaningful baseline for other researchers to build upon requires a more comprehensive evaluation with at least aiming towards finding the best setup. Everything has been done using the same setting. The only parameter is the level of noise. How about: #reframe contribution in intro, abstract, conclusion
* Different dataset size to see if 320k questions are already sufficient (for example, if the results are almost as good with half the volume)
One of the core results seems to be the performance gap of 7% between SpeechMod and TextMod. However, due to the limited evaluation I'm not convinced that this result is in any sense conclusive. To some extend it is expected since TextMod outsources to step of converting audio to in internal representation to much more sophisticated component. While the results in themselves do give some first quantitative insights, they are too preliminary to qualify for ACM Multimedia.

C2: All in all, I fail to see the fundamental difference between the TextMod and SpeechMod architectures in Figure 2. ASR systems such as DeepSpeech2 are based on neural networks. If the authors would, for example, use the vector of the last layer (similar to the 4,096-dim vector of VGG19 for the images), both architectures would look very much alike. What makes having text as intermediate representation so special, apart from being able to be interpreted by humans? To me, the only difference between the two architectures is that TextMod outsources the mapping from audio to "another" representation to be done by an existing component. Whether that resulting representation is text or a vector seems of less relevance. #elaborate on architecture, explain in discussion somewhere why text is better

C4: The presentation of the experiments and results is rather unorthodox and a bit irritating. Usually, an Evaluation section contains the individual experiments together with their results. This also allows to give that section more structure which in turn increases readability. A subsequent Discussion section can then pick up the general results to put them into the bigger picture and motivate future work. In this paper, the results are strewn across Sections 5 and 6, and sections also don't feature a more explicitly structure to connect experiments and results. #redo discussion sections

C8: Given that the paper is intended to "[...] induct speech-based visual question answering [...]" the outline of future work in Section 6.2 (but also the Conclusion sections) feels rather half-hearted. A more structured roadmap that lists different directions to build upon would significantly strengthen the paper. #expand future work

Reviewer 2
In this work, the authors have proposed the integration of speech modality to solve the challenging task of Visual Question Answering. A new dataset is generated by augmenting the existing public dataset (VQA1.0) with speech data. Two different architectures have been proposed by the authors, one is based on text and the other is based on direct integration of speech with the visual modality. The approach is novel for this task where an integration of speech and visual modality is proposed by the authors. Since the performance after integrating speech and visual data is worse as compared to the existing techniques, where text is used instead of speech, the contribution of this work is questionable. The authors were not able to support their idea of integrating speech with visual data for the current task. Another issue is the missing comparison with the existing methods which are focused on VQA task. A novel idea of integrating speech in the task of VQA was the main contribution in this work which is not performing well in the presented evaluation. Apart form this there is no other technical or theoretical contribution in this submission.

Here are some comments for the authors to improve their work,

- In the related work section (2.1) the authors have stated that, "Our work differs from theirs in that we do not try to improve the performance of the VQA model by adding attention mechanisms or augmenting the pooling techniques.". How is this difference useful and what is the significance of this (apart from a simplistic model which is not performing well)? #emphasize focus in related works, or remove

- In the results analysis, it has been stated that, "To logically reason about the results, we make the assumption that spoken language as a modality contains more information than written language." However, the results doesn't support this assumption as the results are worse when audio and visual are combined together. #explain results better

- In the results analysis, the authors pointed out that, "It is important to note the reasoning above and the conclusion does not discourage nor invalidate the end-to-end method.". I will not agree with this, as the results the authors have shown clearly does not support the assumption, and stating something without any solid evidence is mere an hypothesis which needs proper validation and it is missing in this work. There is a 7% gap in the results which is significant and it is not even close to what text based methods provide (improving on that is not even a question here). May be integrating audio with the visual is not a good idea, again this is just an assumption and the results presented here does support this. #revise takeaway to be more neutral and less of an advocate

-  The authors have not compared their proposed method with the existing VQA methods and the reason stated is that they are not trying to improve the existing methods but proposing a new direction which is using speech instead of text. This seems reasonable to some extent. But my major concern is, the integration of speech is not improving the performance of the proposed method. This raises questions on the contribution of this work. #reframe contribution and problem statement

Reviewer 3
The idea of this paper is novel and interesting. At the same time, the authors have conducted comprehensive analysis on the comparison between the two language components, i.e., TextMod and SpeechMod, for the VQA. The analysis could benefit the multimedia community researcher to explore the intersection of speech and vision. However, this paper lacks of research component although the paper is complete in terms of the analysis. Also, as shown in Table 4, the integrating of speech directly with the proposed SpeechMod “performs especially poorly”. It seems the authors just tried a simple and “failed” method, and encourage the readers to try themselves. This further makes the contribution of this paper questionable.
#reframe contribution